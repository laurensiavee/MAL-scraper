import random
import time
import os
import requests
from bs4 import BeautifulSoup
import csv
import pandas as pd
from IPython.display import clear_output

# this script will convert all downloaded HTML files to csv.
# you can configure html file location on config.
# you can configure anime ids.

###################
# CONFIG
###################

# input & output folder
folder = 'data/'
anime_path = folder + 'anime/'
csv_path = folder + 'result/'

html_file = 'anime_'
csv_file = 'anime.csv'

# setting (anime id)
start = 1
end = 70000

# variable used on method to contain all special char
sp_char = ['\u2606', '\u2665', 'ãƒ’ã‚«ãƒªãƒ»æ–°ãŸãªã‚‹æ—…ç«‹ã¡', 'ãƒ‹ãƒ“ã‚¸ãƒ ãƒ»å²ä¸Šæœ€å¤§ã®å±æ©Ÿ', 'ãƒªãƒ«ãƒ«', 'ì¹´í”„ ë°•ì‚¬', 
'\u266a', '\u2605', '\u2060', '\u2661', '\u014d', 'ã‚¦ãƒ«ç¥­', '\u2162', '\u016b', 'æœ—èª­å°‘å¹´', 'æœ—èª­å…„å¼Ÿ', 'ã‚‚ã†ä¸€ã¤ã®ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°~å›šã‚ã‚Œã®å®‰è—¤ãªã¤',
'\uff0d', '\u03bc', 'ã‚¢ãƒ‹ãƒ¡', 'ã‚³ãƒŸãƒƒã‚¯ ãƒ¬ãƒ³ã‚¿ãƒãƒ³', '\u30fb', '\u200b', 'å‚æœ¬æ¸‰å¤ª', '[è¬ã®è€å©†]', '[åå¥‰è¡Œæ–‡ã•ã‚“]', '[å¥³è­¦éƒ¨ç¥å®®å¯ºè‘‰å­]',
'[éˆ´æ£®ãªã‚“ã§ã‚‚ç›¸è«‡æ‰€]', '(', ')', '[', ']', 'å°‘å¥³ç—…', 'ç…§è‹±', 'é®åº§', 'ãƒãƒ£ãƒ³ãƒ—&ã‚¢ãƒƒãƒ—ãƒ«', 'ãƒ¯ãƒ³ãƒ«ãƒ¼ãƒ å™äº‹è©©', 'é£¯é‡çœŸæ¾„', 'ä»Šäº•å¼“å­',
'\u2192', 'æ°¸å€‰æ–°å…«æœ€å¼·ä¼èª¬', 'å¤ã®ã‚ãšã‚‰ã„', '\u03a8', 'ãƒ©ãƒãƒ¼ã‚ºP', 'ãƒã‚«ã˜ã‚ƒå‹ã¦ãªã„ã®ã‚ˆ', 'å°æ²¢æ˜­å·³', 'ã‚­ãƒ«ãƒŸãƒ¼ãƒ™ã‚¤ãƒ™ãƒ¼ã‚¹ãƒ¼ãƒ‘ãƒ¼',
'ç”·å®£è¨€', '\u221e', 'æ½”ç´”çœŸå¤œ', '\u2423', '\u300c', '\u300d', '\u014c', '\u2190', 'â€¦', 'â€•', '\u25ce', '\u03b8', 'ãƒãƒ‰ãƒ³ãƒŠ æ°—æŒã¡ã„ã„äº‹ã—ã¦ä¸‹ã•ã„ã€‚', 'å§‰ï¼†å¦¹ æ°—æŒã¡ã„ã„äº‹ã—ã¦ä¸‹ã•ã„ã€‚',
',', '\u221a', '\u0394', 'ãƒãƒ¼ãƒãƒ¼ãƒ†ã‚¤ãƒ«', 'çŒªå­è‚²ä»£', 'èµ¤çŒ«', 'æ°´çŒ«', 'ã‚«ãƒ«ãƒ', 'éˆ´æœ¨ä¼¸ä¸€', 'ã—ãŠã“ã‚“ã¶', 'é’æ˜¥ãƒŸãƒƒãƒ‰ãƒŠã‚¤ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ã‚º', 'æ½®é¨’',
'\uff0a', 'ã‚­ãƒ‰ã‚¢ã‚¤', 'ã¶ã‚“ã‘ã‹ãª', 'çœŸä¾å­', '\u03b3', '\u03b1', 'æ‰‹å¡šæ²»è™«ä¸–ç´€æœ«ã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸', '\u266d', '\u308b', 'ã®ã‚Šã®ã‚Šã®ã‚Šã‚¹ã‚¿', 'â—¯',
'é’é’è‰åŸ', '\u0101', 'ç™½é›ªã¨ã‚¼ãƒ³ã€ã¯ã˜ã‚ã¦ã®åŸä¸‹ç”ºãƒ‡ãƒ¼ãƒˆ', 'ãƒ©ã‚¸ç‹å­å¹¼å°‘ã®ã¿ãã‚Š', 'å§‹ã¾ã‚Šã®ã‚¼ãƒ³ï¼†ãƒŸãƒ„ãƒ’ãƒ‡ï¼†æœ¨ã€…', '\u2103', '\u30fc', 'å¤äº‹è¨˜ã‚¬ãƒ« æ—¥å‘è·¯ã‚’æ—…ã™',
'æ„›é‡Œå¯¿ã‚¦ã‚©', 'ç§‹å±±å„ªèŠ±é‡Œã®æˆ¦è»Šè¬›åº§', '\uff01', 'ãƒ¡ã‚°ã¨ãƒ‘ãƒˆãƒ­ãƒ³', '\u0103', 'ã‚µã‚«ãƒ¢ãƒˆæ•™æˆ', 'å¤§æœˆå£®', '\u0113', 'ä¸Šé‡æ­¦å¤«', 'é³¥ç¾½å’Œä¸€',
'\u0144', '\u011f', '\u2642', '\u25b3', '\uffe5', '\uff1d', '\u03c7', 'ãƒ‰ãƒ©ãƒ ~ ã‚‚ãã‚Šã“ã‚“ã ã‚‰ ãƒãƒ å¤ªéƒ:', 'ãƒãƒ ã¡ã‚ƒã‚“ãšã‚³ãƒŠ ~ ã¤ã¼ã®ä¸­ã®ãƒ“ã‚¹ã‚±ãƒƒãƒˆ:',
'ãƒ‰ãƒ©ãƒ ~ å¤§å¥½ãã€ãªã¤ã¿ã¡ã‚ƒã‚“:', 'ãƒãƒ ã¡ã‚ƒã‚“ãšã‚³ãƒŠ ~ é»„è‰²ã„ã—ã‚ã‚ã›:', '\u2191', '\u25cb', 'ã‚ã‚ã‰', '\u2934', 'å…«ç‹å­P', '\uff0c',
'\u2260', 'ä¸‰å‰è·¯', 'ãŠã¨ãè©±', 'æ¥ ãƒˆã‚·ã‚¨', 'å·æ©‹å•“å²', 'å¤§å¡šä½³å­', 'æœªæ¥ã®äººã¸', 'ï¼¿', '\u03cc', 'æˆ‘å–œå±‹ä½ç‘³å‹™', 'ãƒã‚¤ã‚¹ãƒ­', '"ã¯ã˜ã¾ã‚Š"',
'çŒªçŒªä¾ ', 'ã‚°ï½ãƒ³', '\u2159', 'ãã‚“ã ã¦ã‚Œã³', '\uff08', '\uff09', '\u25c6', '\u222c', '\u2500', '\ufb01', '\ufb00', 'å¤©å·¥', '\uff5e',
'\u013b', '\uff0b', '\u0179', '\u03b5', '\u03a6', '\u03a9', 'ã‚ºãƒ«ãƒƒã‚°ã¨ãƒŸãƒŸãƒƒã‚­ãƒ¥', 'ãƒ¦ãƒ¡ãƒãƒ„ãƒœãƒŸ', 'ã¾ã£ã¦ã¦ã­ã‚³ã‚¤ã‚­ãƒ³ã‚°', 'ã½ã‹ã½ã‹ãƒã‚°ãƒãƒƒã‚°ãƒã‚¦ã‚¹',
'ãµã¶ãã®ãªã¤ã‚„ã™ã¿', 'ãƒ—ãƒªãƒ³ã®ã†ãŸ', 'ãƒ’ãƒ­ã«ãªã‚ŠãŸã„ãƒ¤ãƒ³ãƒãƒ£ãƒ ', 'ã‚²ãƒ³ã‚¬ã«ãªã£ã¡ã‚ƒã£ãŸï¼Ÿ', '\uff06', '\uff1a', '\u012b', '\u2640', 'é­”æ³•',
'- ã‚·ã‚ªãƒ³', '\uff62', '\uff63', '\u042f', '\u043e', '\ufe0f', '\u25bd', '\u2032', '\u207f', '\u200e', '\u2028', 'æ½Ÿã¡ã‚…ã¶',
'"à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸•à¸µà¹‹à¹„à¸¡à¹ˆà¸¡à¸µà¸§à¸±à¸™à¸¥à¸·à¸¡"', 'ğŸŒŠ', '\u0142', '\u041e']

###################
# STRIPPING DATA
###################
def get_title(soup):
    title = soup.find("h1", {"class": "title-name h1_bold_none"})
    if title is None:
        return "404"
    return title.text.strip()

def get_rating(soup):
    rating = soup.find("span", {"itemprop": "ratingValue"})
    if rating is None:
        return "Unknown"
    return rating.text.strip()

def get_rating_count(soup):
    rating_count = soup.find("span", {"itemprop": "ratingCount"})
    if rating_count is None:
        return "Unknown"
    return rating_count.text.strip()

def get_ranked(soup):
    sub = soup.find("span", {"class": "numbers ranked"})
    if sub is None:
        return "Unknown"
    ranked = sub.find("strong")
    if ranked is None:
        return "Unknown"
    return ranked.text.strip().replace("#", "")

def get_popularity(soup):
    sub = soup.find("span", {"class": "numbers popularity"})
    if sub is None:
        return "Unknown"
    popularity = sub.find("strong")
    if popularity is None:
        return "Unknown"
    return popularity.text.strip().replace("#", "")

def get_members(soup):
    sub = soup.find("span", {"class": "numbers members"})
    if sub is None:
        return "Unknown"
    members = sub.find("strong")
    if members is None:
        return "Unknown"
    return members.text.strip().replace(",", "")

def get_season_aired(soup):
    sub = soup.find("span", {"class": "information members"})
    if sub is None:
        return "Unknown"
    season = sub.find("a")
    if season is None:
        return "Unknown"
    return season.text.strip()

def get_type(soup):
    sub = soup.find("span", {"class": "information type"})
    if sub is None:
        return "Unknown"
    type = sub.find("a")
    if type is None:
        return "Unknown"
    return type.text.strip()

def get_studio(soup):
    sub = soup.find("span", {"class": "information studio author"})
    if sub is None:
        return "Unknown"
    type = sub.find("a")
    if type is None:
        return "Unknown"
    return type.text.strip()

def get_synopsis(soup):
    synopsis = soup.find("p", {"itemprop": "description"})
    if synopsis is None:
        return "Unknown"
    syn = synopsis.text.strip().replace("\n", "").replace("<br/", "").replace("[Written by MAL Rewrite]", "").replace("                            ", "")
    if "(Source:" in syn:
        splitted = syn.split("(Source:")
        syn = splitted[0]
    return syn

def get_eps(soup):
    eps = soup.find("span", {"id": "curEps"})
    if eps is None:
        return "Unknown"
    return eps.text.strip()

def get_genre(soup):
    genres = []

    for genre in soup.find_all("span", {"itemprop": "genre"}):
        genres.append(genre.text.strip())

    if len(genres) == 0:
        return "Unknown"
    return genres

def get_url(soup):
    url = soup.find("meta", {"property": "og:url"})
    return url["content"]

def get_img(soup):
    url = soup.find("meta", {"property": "og:image"})
    return url["content"]

###################
# CONVERT TO CSV
###################

def clean(str):
    for ch in sp_char:
        str = str.replace(ch, "")
    return str

def to_csv(row):
    path = f"{csv_path}"
    os.makedirs(path, exist_ok=True)

    f = open(f"{csv_path}{csv_file}", "a", newline='')
    writer = csv.writer(f)
    writer.writerow(row)
    f.close()

def create_header():
    path = f"{csv_path}"
    os.makedirs(path, exist_ok=True)

    f = open(f"{csv_path}{csv_file}", "w", newline='')

    if os.stat(f"{csv_path}{csv_file}").st_size == 0:
        header = ['anime_id', 'title', 'anime_rating', 'rating_count', 'ranked', 'popularity', 'members', 'season_aired', 'type', 'studio', 'synopsis', 'episode_count', 'genre', 'url', 'img']
        to_csv(header)

def get_info(anime_id):
    with open(f"{anime_path}{html_file}{anime_id}.html", "r", encoding="UTF-8") as file:
        soup = BeautifulSoup(file, "html.parser")

        title = get_title(soup)
        if(title != "404"):
            row = []

            row.append(anime_id)
            row.append(clean(get_title(soup)))
            row.append(clean(get_rating(soup)))
            row.append(clean(get_rating_count(soup)))
            row.append(clean(get_ranked(soup)))
            row.append(clean(get_popularity(soup)))
            row.append(clean(get_members(soup)))
            row.append(clean(get_season_aired(soup)))
            row.append(clean(get_type(soup)))
            row.append(clean(get_studio(soup)))
            row.append(clean(get_synopsis(soup)))
            row.append(clean(get_eps(soup)))
            row.append(get_genre(soup))
            row.append(clean(get_url(soup)))
            row.append(clean(get_img(soup)))

            to_csv(row)

            clear_output(wait=True)
            print("success: " + str(anime_id))

        else:
            clear_output(wait=True)
            print("URL not found: " + str(anime_id))

###################
# RUN SCRIPT
###################

def run():
    create_header()

    for i in range(start, (end + 1)):
        get_info(i)

    print('done')    

    # check csv:
    # df = pd.read_csv(f"{csv_path}{csv_file}", encoding='utf-8')
    # df.head()
    
    # df = df.drop_duplicates(subset='anime_id')
    # df.count()

run()